{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping with Python: An Introductory Tutorial\n",
    "\n",
    "#### Rob Osterburg \n",
    "##### Software Engineer / Instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* The generous [sponsors](http://denverdatascienceday.com/index.php/sponsors/) of [Denver Data Science Day 2017](http://denverdatascienceday.com/)\n",
    "\n",
    "* [Galvanize](https://www.galvanize.com/pick-a-location?page=%2F) for hosting [Denver Data Science Day 2017](http://denverdatascienceday.com/)\n",
    "\n",
    "* Bob Mickus, Tyler B. and all the volunteers from [PyData Denver](https://www.meetup.com/PyData-Denver/) who organized [Denver Data Science Day](http://denverdatascienceday.com/)\n",
    "\n",
    "* Miguel Grinberg whose [Easy Web Scraping with Python](https://blog.miguelgrinberg.com/post/easy-web-scraping-with-python) blog post inspired this tutorial.  Miguel's posted his tutorial in 2014 and PyVideo.org has recently undergone a significant revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* Which Python packages to use\n",
    "* Cover key concepts, tools and techniques\n",
    "* Scrape some data \n",
    "    - Organize it using a namedtuple\n",
    "    - Persist it as a JSON file\n",
    "    - Read it back into a namedtuple and demonstrate semantic equivilence \n",
    "* Share recommended resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Packages\n",
    "\n",
    "* requests -- Issues HTTP requests to web servers and handles the response \n",
    "\n",
    "* beautifulsoup4 -- Parse the returned web page and makes it searchable\n",
    "\n",
    "* These packages are *not* in Python's standard library, here is how to install them:\n",
    " \n",
    "    - Standard Python: `pip install requests beautifulsoup4`\n",
    "\n",
    "    - Anaconda Python: `conda install requests beautifulsoup4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Requests is 'HTTP for Humans'\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup parses and builds tree from HTML\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning! Warning!\n",
    "* Only scrape as a last resort, first see if the site has an API or other means of accessing their data\n",
    "* Web scraping is commonly frowned upon by the site's owners\n",
    "* **Always check the site's Terms of service, Conditions of use, and robots.txt file**   \n",
    "* Aggressive scrapers can take a site down, owner's can rightfully consider that a denial of service attack\n",
    "* If in doubt, talk to a lawyer, especially for *anything* work related\n",
    "* Watch [Sustainable Scrapers, PyData DC, 2016](http://pyvideo.org/pydata-dc-2016/sustainable-scrapers.html) to understand how a data journalist deals with these issues. It's a good talk and quite entertaining -- in a geeky way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org -- Is it OK to scrape?\n",
    "* [PyVideo.org](http://pyvideo.org) provides valuable meta-data about Python presentations\n",
    "* It makes finding talks easy and can be searched by event, presenter, or topic \n",
    "* But what restrictions does it have against scraping?  \n",
    "* Let's take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Terms and Conditions?\n",
    "\n",
    "None that I can see, the site's [About page](http://pyvideo.org/pages/about.html) says:\n",
    "\n",
    "> ... PyVideo.org is a freely available index of freely available resources that seek to provide everyone with the opportunity to learn about Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyVideo.org - Robots.txt?\n",
    "* A robots.txt files contain *restrictions* on the content that web crawlers are permitted to access \n",
    "\n",
    "* The [http://pyvideo.org/robots.txt](http://pyvideo.org/robots.txt) is empty, except for one comment \n",
    "\n",
    "* It appears that there are no restrictions on scraping PyVideo.org  \n",
    "\n",
    "* Let's minimize our impact on the site \n",
    "\n",
    "* To see *The Robots Exclusion Protocol* visit [http://www.robotstxt.org/](http://www.robotstxt.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the Page\n",
    "resp = requests.get('http://pyvideo.org/tags.html')\n",
    "\n",
    "# A status code other than 200 indicates problem of some sort\n",
    "print('status code:', resp.status_code)\n",
    "\n",
    "# A better approach is let requests throw an exception when a problem occurs\n",
    "resp.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HTTP Responses\n",
    "\n",
    "HTTP Verb | Effect | Success | Failure\n",
    "--------- | ------ | ------- | --------\n",
    "POST      | Create | 200     | 400, 40X, 500\n",
    "**GET**       | **Read**   | **200**     | **400, 40X, 500**\n",
    "PUT       | Update | 200     | 400, 40X, 500 \n",
    "DELETE    | Delete | 200     | 400, 40X, 500\n",
    "\n",
    "* [Comprehensive list of HTTP status codes](https://httpstatuses.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the Page\n",
    "data = resp.text\n",
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## BeautifulSoup - Parses the page\n",
    " \n",
    "* Models the page as a tree\n",
    "* Similar to the Document Object Model\n",
    " ![tags page structure](./images/htmltree.png)\n",
    "\n",
    "* Parsers build the tree, pick one\n",
    " - html.parser -> Default parser for Python 3\n",
    " - HTMLParser  -> Default parser for Python 2\n",
    " - lxml        -> Fast requires installing a C library and a PyPI package \n",
    " - html5lib    -> Pure Python and part of the standard library\n",
    " \n",
    " cite: https://interactivepython.org/runestone/static/pythonds/Trees/ExamplesofTrees.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup - Well documented and easy to use API\n",
    "* [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors)('css selector') --> [List of Tags]\n",
    "\n",
    "* [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)(tags, keyword_args, attrs={'attr', 'value'})  --> [List of Tags]\n",
    "    \n",
    "* [find](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find)(tags, keyword_args, attrs={'attr', 'value'}) --> Tag\n",
    "\n",
    "* [BeautifulSoup 4 Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding Page Structure\n",
    "\n",
    "* To extract data from a page, you need to understand its structure\n",
    "* BeautifulSoup holds the tree in memory\n",
    "* Look into it using the our browser's built-in developer tools\n",
    "    - Mac -- Cmd + Opt + i\n",
    "    - Linux -- Ctrl + Shift + i\n",
    "    - Windows -- Ctrl + Shift + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visiting the [Tags page](http://pyvideo.org/tags) you can see the structure of the page  \n",
    "![tags page structure](./images/structure-of-tags-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developer Tools - Getting a CSS selector\n",
    "* Use the element inspector to locate items you want to scrape\n",
    "* Right-click on the element > Copy > Copy CSS Selector\n",
    "* Paste the selector into your code\n",
    "* Make it more general by removing aspects that specific to *a* particular tag\n",
    "* In this case, `nth-child()` and `.col-md-3`\n",
    "* The idea here is to tune the selector to the data you want to scrape\n",
    "* Tutorial: [An Intro to CSS: Finding CSS Selectors](https://dailypost.wordpress.com/2013/07/25/css-selectors/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Build a list of anchor elements using a CSS selector\n",
    "# First we need to clean our CSS using a for-loop\n",
    "topics = []\n",
    "as_copied_css = '#element-list > li:nth-child(1361) > a'\n",
    "cleaned_css = 'li > a'\n",
    "for topic in soup.select(cleaned_css):\n",
    "    topics.append(topic)\n",
    "topics[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why List Comprehensions?\n",
    "* List comprehensions are a powerful way to create lists\n",
    "* For-loop code above == List comprehension below\n",
    "* The CSS selector got us 90% of the way there\n",
    "* List comprehension can do the rest\n",
    "* I find them very handy, I think you will too\n",
    "* Tutorial: [Understanding List Comprehensions in Python 3](https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's using a list comprehension \n",
    "# to build the list of links\n",
    "topics = [topic                                        # adds the topic to the list\n",
    "          for topic in soup.select('li > a')]          # loop over Tags from select\n",
    "topics[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Now lets look at one of the elements in our list\n",
    "* Each element is a BeautifulSoup Tag object\n",
    "* *Tag* allows access to all the element's data including\n",
    "    - *attributes* that are accessed using dictionary like syntax\n",
    "    - *contents* that are displayed on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look a Tag instance\n",
    "topic = topics[7]\n",
    "# Access the data it holds\n",
    "type(topic), topic, topic.contents, topic['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* List comprehensions are concise and powerful to process data\n",
    "* From the list of anchor elements\n",
    "* Let's create a list of links to topics pages\n",
    "* Filtering out all unrelated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting and filtering the links\n",
    "topics = [topic['href']                       # extract attribute value\n",
    "          for topic in soup.select('li > a')  # loop over anchors\n",
    "          if '/tag/' in topic['href']]        # filter\n",
    "         \n",
    "len(topics), type(topics[0]), topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the page\n",
    "resp = requests.get('http://pyvideo.org/tag/scraping')\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the page\n",
    "data = resp.text\n",
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Tag.find_all()\n",
    "* We have used `select()`, not let's try out `find_all()` \n",
    "* Our goal is create a list of the links to all the talks on a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Find all the video page links using find_all()\n",
    "h4_tags = soup.find_all('h4', class_='entry-title')\n",
    "h4_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup in action\n",
    "* Methods: `find_all()` and `select()` both return list of Tag objects\n",
    "* Accessing descendents:  Tag.Descendent_Tag['attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the link from <a> tag inside the <h4>\n",
    "findall_talks = [tag.a['href'] for tag in h4_tags]\n",
    "len(findall_talks), findall_talks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Same thing using select() -- In this case select seems cleaner\n",
    "site_url = 'http://pyvideo.org'\n",
    "select_talks = [site_url + tag['href'] \n",
    "                for tag in soup.select('article > section > h4 > a')]\n",
    "len(select_talks), select_talks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving on to extract presentation meta data\n",
    "* Our ultimate destination is now in sight -- The video's presentation page\n",
    "* It's the talk page -- that's where the best data are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get a page and parse it \n",
    "resp = requests.get(select_talks[0])\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "soup.select('.entry-title > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the talk's title\n",
    "titles = [title.contents[0].strip() \n",
    "          for title in soup.select('.entry-title > a')]\n",
    "title = titles[0]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the speaker's name\n",
    "names = [elem.contents[0] \n",
    "         for elem in soup.select('.url')]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the talk details\n",
    "details = [detail \n",
    "           for detail in soup.select('.details-content > ul > li > a')]\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the YouTube link\n",
    "links = [detail['href'] \n",
    "         for detail in details \n",
    "         if 'youtube.com' in detail['href']]\n",
    "link = links[0]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the subject tags\n",
    "tags = [link.contents[0].lower() \n",
    "        for link in details \n",
    "        if 'tag' in link['href']]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the description\n",
    "paragraphs = soup.select('.entry-content > p')\n",
    "description = '\\n\\n'.join([p.contents[0] for p in paragraphs])\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Saving the Data\n",
    "\n",
    "* Use namedtuple to capture information about PyVideo talk\n",
    "\n",
    "* What do we need to capture\n",
    "    - Talk title (string)\n",
    "    - Name of presenter(s) (list)\n",
    "    - Description (string)\n",
    "    - Tags (list)\n",
    "    - Link to video (string)\n",
    "    \n",
    "* Let's use JSON \n",
    "    - Awesome for persisting structured data \n",
    "    - Excellent data exchange format\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why namedtuples?\n",
    "* namedtuples are a lightweight container for data\n",
    "* Using a name to access data elements is their advantage\n",
    "* Numeric indexing is all that regular tuple provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Storing related data in a namedtuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# create the structure\n",
    "pyvideo = namedtuple('pyvideo', 'title names description tags link')\n",
    "\n",
    "# create an instance\n",
    "talk_data = pyvideo(title=title, names=names, tags=tags, link=link, description=description)\n",
    "\n",
    "# access the data and display it as a formatted string\n",
    "fmt = 'title={}\\nnames={}\\ntags={}\\nlink={}'\n",
    "print(fmt.format(talk_data.title, talk_data.names, talk_data.tags, talk_data.link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why JSON?\n",
    "* Handles structured data well\n",
    "* Excellent data exchange format\n",
    "* Python supports round-tripping between namedtuples and JSON\n",
    "    - namedtuple objects can be saved a JSON formatted text\n",
    "    - JSON formatted text can be convert back to namedtuple objects\n",
    "    - Of course, text can be written to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Writing the namedtuple to disk as JSON\n",
    "import json\n",
    "\n",
    "# Write to file\n",
    "with open('a_video.json', 'w') as fout:\n",
    "    # Note: preserve keys by saving a dictionary, not a list\n",
    "    json.dump(talk_data._asdict(), fout)\n",
    "    \n",
    "# Read from file\n",
    "with open('a_video.json', 'r') as fin:\n",
    "    restored_talk_data = json.load(fin)\n",
    "\n",
    "# Deserialize -- Note: pass as keyword arguments using **\n",
    "restored_talk_data = pyvideo(**restored_talk_data)\n",
    "\n",
    "# Compare semantically\n",
    "talk_data == restored_talk_data\n",
    "type(talk_data), type(restored_talk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing web scraping code has advantages\n",
    "\n",
    "* Excellent source of valuable data \n",
    "\n",
    "* The code is approachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ... and presents some problems too\n",
    "\n",
    "* Your code breaks when the site is updated\n",
    "\n",
    "* Read the terms and conditions, otherwise ...\n",
    "\n",
    "* Getting blocked, banned, sued ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "1. [Automate the Boring Stuff](https://automatetheboringstuff.com/), [Chapter 11 — Web Scraping](https://automatetheboringstuff.com/chapter11/) by Al Sweigart (Free PDF version online).  Takes you through topics step-by-step, includes using Selenium to fill out forms and simulate mouse clicks. \n",
    "\n",
    "1. [RealPython Blog -- Web Scraping With Scrapy and MongoDB](https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/) by Micheal Herman. Scrapy is a Python package that makes scraping code easier to maintain. \n",
    "\n",
    "1. [Talk Python to Me Podcast -- Web scraping at scale with Scrapy and ScrapingHub](https://talkpython.fm/episodes/show/50/web-scraping-at-scale-with-scrapy-and-scrapinghub). Web scraping as a Service from the author of Scrapy.\n",
    "\n",
    "1. [PyVideo.org](http://pyvideo.org/)— Comprehensive catalog of videos of over 8000 of Python related presentations. Talks on scraping web pages can be found on the [Scraping page](http://pyvideo.org/tag/scraping/). \n",
    "\n",
    "1. [Web Scraping with Python: Collecting Data from the Modern Web](https://www.amazon.com/Web-Scraping-Python-Collecting-Modern/dp/1491910291) by Ryan Mitchell.  This 4.5 star book on Amazon covers scraping topics in depth.\n",
    "\n",
    "1. [Awesome Python](https://awesome-python.com/) -- PyPI has over 100,000 packages.  Awesome Python is a curated list of the best, see their recommended web scraping packages [here](https://awesome-python.com/#web-crawling).\n",
    "\n",
    "1. Other Practice Sites\n",
    "    * [Books to Scrape](http://books.toscrape.com/)\n",
    "    * [Quotes to Scrape](http://quotes.toscrape.com/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Goes to Wikipedia to get ticker symbols, and CIK codes for S&P 500 companies\n",
    "'''\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "r = requests.get(url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "company_table = soup.find_all('tr')[1:506]\n",
    "company_dict = dict()\n",
    "\n",
    "for row in company_table:\n",
    "    row = row.find_all('td')\n",
    "    # slice into table to grab ticker and CIK\n",
    "    company_dict[row[0].get_text()] = row[7].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001018724&type=8-k&dateb=20180101&owner=exclude&count=10\n",
    "\n",
    "ticker = 'AMZN'\n",
    "cik, priorto, count = company_dict[ticker], 20180101, 10\n",
    "base_url = 'http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK='\\\n",
    "    +str(cik)+'&type=8-K&dateb='\\\n",
    "    +str(priorto)+'&owner=exclude&output=xml&count='\\\n",
    "    +str(count)\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL for each 8-K detail page using the 'filinghref' html tag:\n",
    "detail_urls = soup.find_all('filinghref')\n",
    "detail_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = []\n",
    "\n",
    "# Iterate through each URL:\n",
    "for url in detail_urls:\n",
    "    # If URL suffix is 'htm', change to 'html'\n",
    "    url = url.string\n",
    "    if url.split('.')[len(url.string.split('.')) - 1] == 'htm':\n",
    "        url+='l'\n",
    "            \n",
    "    # Turn each URL into soup\n",
    "    r = requests.get(url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    \n",
    "    # Locate the filing date from the detail page by searching by html tag\n",
    "    filing_date = soup.find_all('div', {\"class\" : 'info'})[1].string\n",
    "    \n",
    "    # The suffix to the form 8-K can be found on this page too\n",
    "    url_suffix = soup.find_all('tr')[1].find_all('td')[2].get_text()\n",
    "    \n",
    "    # Slice off the end of our url, and add the new suffix to get the Form 8-K url\n",
    "    doc_url = url[0:-31] + url_suffix\n",
    "    \n",
    "    # Turn the Form 8-k url into soup\n",
    "    r = requests.get(doc_url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    \n",
    "    # Finally, the text we're after! \n",
    "    # Get rid of the commas, quotation marks, and any new lines characters\n",
    "    text = soup.get_text().replace(',', '').replace('\"', '').replace('\\n', ' ')\n",
    "    \n",
    "    # Combine the data into a CSV style string and write to the file created before the for loop started\n",
    "    write_string = ticker + ',' + filing_date + ',' + text + '\\n'\n",
    "    \n",
    "    forms.append(write_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(detail_urls[0].string)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "filing_date = soup.find_all('div', {\"class\" : 'info'})[1].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
